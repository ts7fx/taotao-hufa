"""DataForSEO API é›†æˆæ¨¡å—

DataForSEO æä¾›ä¸“ä¸šçš„ SEO æ•°æ®åˆ†æï¼š
- OnPage API: é¡µé¢çº§ SEO å®¡è®¡ã€Core Web Vitalsã€æŠ€æœ¯ SEO æ£€æŸ¥
- Backlinks API: åå‘é“¾æ¥åˆ†æã€å¼•ç”¨åŸŸåã€é”šæ–‡æœ¬
- SERP API: å…³é”®è¯æ’åè¿½è¸ª

è®¤è¯æ–¹å¼: HTTP Basic Auth
Base URL: https://api.dataforseo.com/v3/
æ–‡æ¡£: https://docs.dataforseo.com/v3/
å®šä»·: æŒ‰é‡ä»˜è´¹ï¼ˆPay-As-You-Goï¼‰ï¼ŒSandbox å…è´¹

ä½¿ç”¨æ–¹å¼:
    python main.py https://tenmomo.com --dataforseo-login YOUR_LOGIN --dataforseo-password YOUR_PASSWORD

ç¯å¢ƒå˜é‡:
    DATAFORSEO_LOGIN=your_login
    DATAFORSEO_PASSWORD=your_password
"""

import json
import requests
from models import Finding, Category, Severity


class DataForSEOClient:
    """DataForSEO API å®¢æˆ·ç«¯"""

    BASE_URL = "https://api.dataforseo.com/v3"

    def __init__(self, login: str, password: str, sandbox: bool = False):
        self.auth = (login, password)
        if sandbox:
            self.BASE_URL = "https://sandbox.dataforseo.com/v3"
        self.session = requests.Session()
        self.session.auth = self.auth
        self.session.headers.update({
            "Content-Type": "application/json",
        })

    def _post(self, endpoint: str, data: list[dict]) -> dict:
        """å‘é€ POST è¯·æ±‚"""
        url = f"{self.BASE_URL}{endpoint}"
        resp = self.session.post(url, json=data, timeout=60)
        resp.raise_for_status()
        return resp.json()

    def _get(self, endpoint: str) -> dict:
        """å‘é€ GET è¯·æ±‚"""
        url = f"{self.BASE_URL}{endpoint}"
        resp = self.session.get(url, timeout=60)
        resp.raise_for_status()
        return resp.json()

    def instant_pages(self, url: str) -> dict | None:
        """å®æ—¶å•é¡µåˆ†æ - è¿”å› onpage_scoreã€Core Web Vitalsã€SEO æ£€æŸ¥é¡¹

        æ–‡æ¡£: https://docs.dataforseo.com/v3/on_page-instant_pages/
        è´¹ç”¨: æŒ‰çˆ¬å–é¡µé¢æ•° + å‚æ•°é™„åŠ è´¹
        é€Ÿç‡é™åˆ¶: 2000 è¯·æ±‚/åˆ†é’Ÿï¼Œ30 å¹¶å‘

        è¿”å›ç»“æ„: result[0].items[0] åŒ…å«é¡µé¢çº§æ•°æ®
        """
        data = [{
            "url": url,
            "enable_javascript": True,
            "load_resources": True,
            "check_spell": True,
            "validate_micromarkup": True,
        }]
        try:
            resp = self._post("/on_page/instant_pages", data)
            if resp.get("status_code") == 20000 and resp.get("tasks"):
                task = resp["tasks"][0]
                if task.get("status_code") == 20000 and task.get("result"):
                    result = task["result"][0]
                    # å®é™…é¡µé¢æ•°æ®åœ¨ items æ•°ç»„ä¸­
                    items = result.get("items", [])
                    if items:
                        return items[0]
                    return result
        except Exception as e:
            print(f"  âš ï¸ DataForSEO instant_pages å¤±è´¥: {e}")
        return None

    def backlinks_summary(self, target: str) -> dict | None:
        """åå‘é“¾æ¥æ¦‚å†µ

        æ–‡æ¡£: https://docs.dataforseo.com/v3/backlinks-overview/
        è´¹ç”¨: $0.02/è¯·æ±‚ + $0.00003/æ¯è¡Œæ•°æ®
        æ³¨æ„: Backlinks API æœ‰ $100/æœˆæœ€ä½æ¶ˆè´¹ï¼ˆMake.com/n8n é™¤å¤–ï¼‰
        """
        data = [{
            "target": target,
            "exclude_internal_backlinks": True,
        }]
        try:
            resp = self._post("/backlinks/summary/live", data)
            if resp.get("status_code") == 20000 and resp.get("tasks"):
                task = resp["tasks"][0]
                if task.get("status_code") == 20000 and task.get("result"):
                    return task["result"][0] if isinstance(task["result"], list) else task["result"]
                elif task.get("status_code") == 40204:
                    print(f"  âš ï¸ Backlinks API æœªå¼€é€šï¼ˆéœ€è¦è®¢é˜…ï¼‰")
                    return None
        except Exception as e:
            print(f"  âš ï¸ DataForSEO backlinks_summary å¤±è´¥: {e}")
        return None


class DataForSEOAnalyzer:
    """åˆ©ç”¨ DataForSEO API è¿›è¡Œæ·±åº¦åˆ†æ"""

    def __init__(self, client: DataForSEOClient, target_url: str):
        self.client = client
        self.target_url = target_url
        # ä» URL æå–åŸŸå
        from urllib.parse import urlparse
        self.domain = urlparse(target_url).netloc

    def analyze(self) -> list[Finding]:
        findings = []

        # 1. é¡µé¢çº§åˆ†æ
        print("  ğŸ“¡ DataForSEO: åˆ†æé¦–é¡µ...")
        page_data = self.client.instant_pages(self.target_url)
        if page_data:
            findings.extend(self._parse_instant_pages(page_data))
        else:
            findings.append(Finding(
                category=Category.SEO, severity=Severity.INFO,
                title="DataForSEO é¡µé¢åˆ†æä¸å¯ç”¨",
                description="æ— æ³•é€šè¿‡ DataForSEO API è·å–é¡µé¢æ•°æ®",
            ))

        # 2. åé“¾åˆ†æ
        print("  ğŸ“¡ DataForSEO: åˆ†æåå‘é“¾æ¥...")
        backlinks = self.client.backlinks_summary(self.domain)
        if backlinks:
            findings.extend(self._parse_backlinks(backlinks))

        return findings

    def _parse_instant_pages(self, data: dict) -> list[Finding]:
        findings = []

        # OnPage è¯„åˆ†
        onpage_score = data.get("onpage_score")
        if onpage_score is not None:
            if onpage_score >= 80:
                severity = Severity.GOOD
            elif onpage_score >= 60:
                severity = Severity.WARNING
            else:
                severity = Severity.ERROR
            findings.append(Finding(
                category=Category.SEO, severity=severity,
                title=f"[DataForSEO] OnPage è¯„åˆ†: {onpage_score:.1f}/100",
                description="åŸºäº DataForSEO ä¸“ä¸šè¯„ä¼°çš„é¡µé¢ä¼˜åŒ–å¾—åˆ†",
                url=self.target_url,
            ))

        # æ£€æŸ¥é¡¹ï¼ˆtrue = é—®é¢˜å­˜åœ¨ï¼‰
        checks = data.get("checks", {})
        # è¿™äº› check çš„ true è¡¨ç¤ºæœ‰é—®é¢˜
        problem_checks = {
            "no_title": ("ç¼ºå°‘ title æ ‡ç­¾", Severity.ERROR, Category.SEO),
            "no_description": ("ç¼ºå°‘ meta description", Severity.WARNING, Category.SEO),
            "no_h1_tag": ("ç¼ºå°‘ H1 æ ‡ç­¾", Severity.ERROR, Category.SEO),
            "has_meta_refresh_redirect": ("ä½¿ç”¨äº† meta refresh é‡å®šå‘", Severity.WARNING, Category.SEO),
            "is_broken": ("é¡µé¢å·²æŸå", Severity.ERROR, Category.CONTENT),
            "no_image_alt": ("å›¾ç‰‡ç¼ºå°‘ alt å±æ€§", Severity.WARNING, Category.SEO),
            "no_image_title": ("å›¾ç‰‡ç¼ºå°‘ title å±æ€§", Severity.INFO, Category.SEO),
            "no_favicon": ("ç¼ºå°‘ favicon", Severity.WARNING, Category.SEO),
            "no_content_encoding": ("æœªå¯ç”¨å†…å®¹å‹ç¼©", Severity.WARNING, Category.PERFORMANCE),
            "high_loading_time": ("é¡µé¢åŠ è½½æ—¶é—´è¿‡é•¿", Severity.ERROR, Category.PERFORMANCE),
            "is_http": ("ä½¿ç”¨ HTTP è€Œé HTTPS", Severity.ERROR, Category.SECURITY),
            "low_content_rate": ("å†…å®¹å æ¯”è¿‡ä½ï¼ˆçº¯æ–‡æœ¬å æ¯”ä½ï¼‰", Severity.WARNING, Category.CONTENT),
            "high_waiting_time": ("æœåŠ¡å™¨ç­‰å¾…æ—¶é—´è¿‡é•¿", Severity.WARNING, Category.PERFORMANCE),
            "no_doctype": ("ç¼ºå°‘ DOCTYPE å£°æ˜", Severity.WARNING, Category.SEO),
            "title_too_short": ("title æ ‡ç­¾è¿‡çŸ­", Severity.WARNING, Category.SEO),
            "title_too_long": ("title æ ‡ç­¾è¿‡é•¿", Severity.WARNING, Category.SEO),
            "has_render_blocking_resources": ("å­˜åœ¨æ¸²æŸ“é˜»å¡èµ„æº", Severity.WARNING, Category.PERFORMANCE),
            "https_to_http_links": ("HTTPS é¡µé¢åŒ…å« HTTP é“¾æ¥", Severity.ERROR, Category.SECURITY),
            "size_greater_than_3mb": ("é¡µé¢å¤§äº 3MB", Severity.ERROR, Category.PERFORMANCE),
            "duplicate_title_tag": ("é‡å¤çš„ title æ ‡ç­¾", Severity.WARNING, Category.SEO),
            "duplicate_meta_tags": ("é‡å¤çš„ meta æ ‡ç­¾", Severity.WARNING, Category.SEO),
            "deprecated_html_tags": ("ä½¿ç”¨äº†å·²åºŸå¼ƒçš„ HTML æ ‡ç­¾", Severity.WARNING, Category.SEO),
        }

        for check_name, is_flagged in checks.items():
            if is_flagged and check_name in problem_checks:
                desc, sev, cat = problem_checks[check_name]
                findings.append(Finding(
                    category=cat, severity=sev,
                    title=f"[DataForSEO] {desc}",
                    description=f"DataForSEO æ£€æµ‹åˆ°: {check_name}",
                    url=self.target_url,
                ))

        # é¡µé¢å…ƒæ•°æ®æ‘˜è¦
        meta = data.get("meta", {})
        if meta:
            title = meta.get("title", "")
            desc_len = meta.get("description_length", 0)
            content_info = meta.get("content", {})
            details = []
            if title:
                details.append(f"title: '{title}' ({meta.get('title_length', 0)} å­—ç¬¦)")
            details.append(f"meta description: {desc_len} å­—ç¬¦")
            if content_info:
                details.append(f"çº¯æ–‡æœ¬: {content_info.get('plain_text_word_count', 0)} è¯")
                details.append(f"å¯è¯»æ€§ (Flesch): {content_info.get('flesch_kincaid_readability_index', 0):.1f}")
                details.append(f"å†…å®¹å æ¯”: {content_info.get('plain_text_rate', 0) * 100:.1f}%")
            social = meta.get("social_media_tags", {})
            if social:
                details.append(f"OG æ ‡ç­¾: {len(social)} ä¸ª")
            findings.append(Finding(
                category=Category.SEO, severity=Severity.INFO,
                title="[DataForSEO] é¡µé¢å…ƒæ•°æ®æ‘˜è¦",
                description="\n".join(f"  {d}" for d in details),
                url=self.target_url,
            ))

        # Core Web Vitals / é¡µé¢æ€§èƒ½
        page_timing = data.get("page_timing", {})
        if page_timing:
            time_to_interactive = page_timing.get("time_to_interactive")
            if time_to_interactive:
                tti_sec = time_to_interactive / 1000
                if tti_sec > 5:
                    severity = Severity.ERROR
                elif tti_sec > 3:
                    severity = Severity.WARNING
                else:
                    severity = Severity.GOOD
                findings.append(Finding(
                    category=Category.PERFORMANCE, severity=severity,
                    title=f"[DataForSEO] Time to Interactive: {tti_sec:.2f}s",
                    description="é¡µé¢å¯äº¤äº’æ‰€éœ€æ—¶é—´ï¼ˆ<3s è‰¯å¥½ï¼Œ>5s éœ€ä¼˜åŒ–ï¼‰",
                    url=self.target_url,
                ))

            dom_complete = page_timing.get("dom_complete")
            if dom_complete:
                dom_sec = dom_complete / 1000
                if dom_sec > 5:
                    severity = Severity.ERROR
                elif dom_sec > 3:
                    severity = Severity.WARNING
                else:
                    severity = Severity.GOOD
                findings.append(Finding(
                    category=Category.PERFORMANCE, severity=severity,
                    title=f"[DataForSEO] DOM Complete: {dom_sec:.2f}s",
                    description="DOM åŠ è½½å®Œæˆæ—¶é—´",
                    url=self.target_url,
                ))

            # é¡µé¢æ€»å¤§å°
            total_size = data.get("total_dom_size", 0)
            encoded_size = data.get("encoded_size", 0)
            if total_size:
                findings.append(Finding(
                    category=Category.PERFORMANCE, severity=Severity.INFO,
                    title=f"[DataForSEO] é¡µé¢å¤§å°: {total_size / 1024:.1f}KB (å‹ç¼©å {encoded_size / 1024:.1f}KB)",
                    description=f"å‹ç¼©ç¼–ç : {data.get('content_encoding', 'none')}",
                    url=self.target_url,
                ))

        return findings

    def _parse_backlinks(self, data: dict) -> list[Finding]:
        findings = []

        total_backlinks = data.get("backlinks", 0)
        referring_domains = data.get("referring_domains", 0)
        rank = data.get("rank", 0)
        broken_backlinks = data.get("broken_backlinks", 0)

        # æ€»åé“¾æ•°
        if total_backlinks > 100:
            severity = Severity.GOOD
        elif total_backlinks > 10:
            severity = Severity.WARNING
        else:
            severity = Severity.ERROR
        findings.append(Finding(
            category=Category.SEO, severity=severity,
            title=f"[DataForSEO] æ€»åå‘é“¾æ¥æ•°: {total_backlinks}",
            description=f"å¼•ç”¨åŸŸå: {referring_domains}, Domain Rank: {rank}",
            recommendation="é€šè¿‡ä¼˜è´¨å†…å®¹è¥é”€å’Œå¤–é“¾å»ºè®¾å¢åŠ é«˜è´¨é‡åé“¾" if total_backlinks < 100 else "",
        ))

        # åé“¾
        if broken_backlinks > 0:
            findings.append(Finding(
                category=Category.CONTENT, severity=Severity.WARNING,
                title=f"[DataForSEO] å‘ç° {broken_backlinks} ä¸ªå¤±æ•ˆåå‘é“¾æ¥",
                description="éƒ¨åˆ†æŒ‡å‘æœ¬ç«™çš„å¤–éƒ¨é“¾æ¥å·²å¤±æ•ˆ",
                recommendation="è”ç³»é“¾æ¥æ¥æºæ›´æ–° URL æˆ–è®¾ç½® 301 é‡å®šå‘",
            ))

        return findings
