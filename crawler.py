"""çˆ¬è™«æ ¸å¿ƒï¼šBFS çˆ¬å–ã€é“¾æ¥å‘ç°ã€é€Ÿç‡æ§åˆ¶"""

import time
import re
from collections import deque
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser

import requests
from bs4 import BeautifulSoup

from models import PageData
from utils import normalize_url, is_same_domain, is_crawlable_url


class Crawler:
    USER_AGENT = "TaoTaoHuFa/1.0"

    def __init__(self, base_url: str, max_pages: int = 50, delay: float = 1.0):
        self.base_url = base_url.rstrip("/")
        self.max_pages = max_pages
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": self.USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
        })
        self.visited: set[str] = set()
        self.pages: dict[str, PageData] = {}
        self.robots_parser = self._load_robots()

    def _load_robots(self) -> RobotFileParser:
        """åŠ è½½ robots.txt"""
        rp = RobotFileParser()
        robots_url = f"{self.base_url}/robots.txt"
        try:
            rp.set_url(robots_url)
            rp.read()
            print(f"  âœ“ robots.txt å·²åŠ è½½")
        except Exception:
            print(f"  âœ— robots.txt åŠ è½½å¤±è´¥ï¼Œå°†çˆ¬å–æ‰€æœ‰é¡µé¢")
        return rp

    def _can_fetch(self, url: str) -> bool:
        """æ£€æŸ¥ robots.txt æ˜¯å¦å…è®¸çˆ¬å–"""
        try:
            return self.robots_parser.can_fetch(self.USER_AGENT, url)
        except Exception:
            return True

    def _fetch_page(self, url: str) -> PageData:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        page = PageData(url=url)
        try:
            start = time.time()
            resp = self.session.get(url, timeout=15, allow_redirects=True)
            page.response_time = time.time() - start
            page.status_code = resp.status_code
            page.content_type = resp.headers.get("Content-Type", "")
            page.headers = dict(resp.headers)
            page.content_length = len(resp.content)

            if "text/html" not in page.content_type:
                return page

            page.html = resp.text
            self._parse_html(page)

        except requests.RequestException as e:
            page.error = str(e)
            page.status_code = 0

        return page

    def _parse_html(self, page: PageData):
        """è§£æ HTMLï¼Œæå–ç»“æ„åŒ–æ•°æ®"""
        soup = BeautifulSoup(page.html, "html.parser")

        # title
        title_tag = soup.find("title")
        page.title = title_tag.get_text(strip=True) if title_tag else ""

        # meta description
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc:
            page.meta_description = meta_desc.get("content", "")

        # H æ ‡ç­¾
        page.h1_tags = [h.get_text(strip=True) for h in soup.find_all("h1")]
        page.h2_tags = [h.get_text(strip=True) for h in soup.find_all("h2")]
        page.h3_tags = [h.get_text(strip=True) for h in soup.find_all("h3")]

        # å›¾ç‰‡ï¼ˆè¯†åˆ« <picture> + <source type="image/webp"> çš„æ ‡å‡†å†™æ³•ï¼‰
        for img in soup.find_all("img"):
            has_webp_source = False
            parent = img.parent
            if parent and parent.name == "picture":
                for source in parent.find_all("source"):
                    stype = (source.get("type") or "").lower()
                    srcset = (source.get("srcset") or "").lower()
                    if "webp" in stype or srcset.endswith(".webp"):
                        has_webp_source = True
                        break
            page.images.append({
                "src": img.get("src", ""),
                "alt": img.get("alt", ""),
                "has_webp_source": has_webp_source,
            })

        # é“¾æ¥
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith(("mailto:", "tel:", "javascript:", "#")):
                continue
            full_url = normalize_url(href, page.url)
            if is_same_domain(full_url, self.base_url):
                page.internal_links.append(full_url)
            else:
                page.external_links.append(full_url)

        # canonical
        canonical = soup.find("link", rel="canonical")
        if canonical:
            page.canonical_url = canonical.get("href", "")

        # Open Graph
        for og in soup.find_all("meta", property=re.compile(r"^og:")):
            page.og_tags[og["property"]] = og.get("content", "")

        # JSON-LD
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                import json
                data = json.loads(script.string or "")
                if isinstance(data, list):
                    page.json_ld.extend(data)
                else:
                    page.json_ld.append(data)
            except (json.JSONDecodeError, TypeError):
                pass

        # èµ„æºç»Ÿè®¡
        page.scripts = [s.get("src", "") for s in soup.find_all("script", src=True)]
        page.stylesheets = [
            l.get("href", "") for l in soup.find_all("link", rel="stylesheet")
        ]

        # å­—æ•°ï¼ˆçº¯æ–‡æœ¬å†…å®¹ï¼‰
        for tag in soup(["script", "style", "nav", "header", "footer"]):
            tag.decompose()
        text = soup.get_text(separator=" ", strip=True)
        # ä¸­æ–‡æŒ‰å­—ç¬¦ç®—ï¼Œè‹±æ–‡æŒ‰å•è¯ç®—
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        page.word_count = chinese_chars + english_words

    def crawl(self) -> dict[str, PageData]:
        """BFS å¹¿åº¦ä¼˜å…ˆçˆ¬å–"""
        print(f"\nğŸ•·ï¸ æ¡ƒæ¡ƒæŠ¤æ³• - å¼€å§‹çˆ¬å– {self.base_url}")
        print(f"  æœ€å¤§é¡µé¢æ•°: {self.max_pages}, è¯·æ±‚é—´éš”: {self.delay}s\n")

        queue = deque([self.base_url])
        self.visited.add(self.base_url)

        while queue and len(self.pages) < self.max_pages:
            url = queue.popleft()

            if not self._can_fetch(url):
                print(f"  [è·³è¿‡] robots.txt ç¦æ­¢: {url}")
                continue

            print(f"  [{len(self.pages) + 1}/{self.max_pages}] {url}", end=" ")
            page = self._fetch_page(url)
            self.pages[url] = page

            if page.error:
                print(f"âŒ é”™è¯¯: {page.error}")
            elif page.status_code != 200:
                print(f"âš ï¸ {page.status_code}")
            else:
                print(f"âœ… {page.response_time:.1f}s {page.word_count}å­—")

            # å‘ç°æ–°é“¾æ¥åŠ å…¥é˜Ÿåˆ—
            for link in page.internal_links:
                if (link not in self.visited
                        and is_crawlable_url(link)
                        and len(self.visited) < self.max_pages * 2):
                    self.visited.add(link)
                    queue.append(link)

            if queue and len(self.pages) < self.max_pages:
                time.sleep(self.delay)

        print(f"\nâœ… çˆ¬å–å®Œæˆï¼Œå…± {len(self.pages)} ä¸ªé¡µé¢\n")
        return self.pages
