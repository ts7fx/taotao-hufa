"""æ¡ƒæ¡ƒæŠ¤æ³• - ç½‘ç«™åˆ†æçˆ¬è™« CLI å…¥å£"""

import argparse
import os
import sys
import time
from datetime import datetime
from urllib.parse import urlparse

from crawler import Crawler
from analyzers import SEOAnalyzer, PerformanceAnalyzer, ContentAnalyzer, SecurityAnalyzer
from models import AnalysisReport, CategoryReport, Category
from report import generate_report
from utils import calculate_score, score_to_grade, now_str


def run_analysis(url: str, max_pages: int = 50, delay: float = 1.0,
                 output: str = "report.html",
                 dataforseo_login: str = "", dataforseo_password: str = ""):
    """æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹"""
    print("ğŸ‘ æ¡ƒæ¡ƒæŠ¤æ³• v1.0 â€” tenmomo.com çš„å®ˆæŠ¤å·¥å…·")
    print("=" * 50)

    # 1. çˆ¬å–
    start_time = time.time()
    crawler = Crawler(url, max_pages=max_pages, delay=delay)
    pages = crawler.crawl()

    if not pages:
        print("âŒ æœªçˆ¬å–åˆ°ä»»ä½•é¡µé¢ï¼Œè¯·æ£€æŸ¥ URL")
        sys.exit(1)

    crawl_duration = time.time() - start_time

    # 2. å››ç»´åˆ†æ
    print("ğŸ” å¼€å§‹åˆ†æ...")
    analyzers = [
        (Category.SEO, SEOAnalyzer(pages, url)),
        (Category.PERFORMANCE, PerformanceAnalyzer(pages, url)),
        (Category.CONTENT, ContentAnalyzer(pages, url)),
        (Category.SECURITY, SecurityAnalyzer(pages, url)),
    ]

    # DataForSEO å¢å¼ºåˆ†æï¼ˆå¯é€‰ï¼‰
    dataforseo_findings = []
    if dataforseo_login and dataforseo_password:
        print("\nğŸ“¡ DataForSEO API å¢å¼ºåˆ†æ...")
        try:
            from analyzers.dataforseo import DataForSEOClient, DataForSEOAnalyzer
            client = DataForSEOClient(dataforseo_login, dataforseo_password)
            dfs_analyzer = DataForSEOAnalyzer(client, url)
            dataforseo_findings = dfs_analyzer.analyze()
            print(f"  è·å¾— {len(dataforseo_findings)} æ¡ä¸“ä¸šåˆ†æç»“æœ")
        except Exception as e:
            print(f"  âš ï¸ DataForSEO åˆ†æå¤±è´¥: {e}")
    else:
        print("\nğŸ’¡ æç¤º: é…ç½® DataForSEO API å¯è·å¾—æ›´ä¸“ä¸šçš„åˆ†æ")
        print("   --dataforseo-login LOGIN --dataforseo-password PASSWORD")
        print("   æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ DATAFORSEO_LOGIN / DATAFORSEO_PASSWORD")
        print("   æ–‡æ¡£: https://docs.dataforseo.com/v3/\n")

    categories = []
    for category, analyzer in analyzers:
        findings = analyzer.analyze()
        # åˆå¹¶ DataForSEO åŒç±»åˆ«çš„å‘ç°
        for df in dataforseo_findings:
            if df.category == category:
                findings.append(df)
        score = calculate_score(findings)
        grade = score_to_grade(score)
        cat_report = CategoryReport(
            category=category,
            score=score,
            grade=grade,
            findings=findings,
        )
        categories.append(cat_report)
        print(f"  {category.value}: {score} åˆ† ({grade})")

    # 3. ç»¼åˆè¯„åˆ†
    overall_score = sum(c.score for c in categories) // len(categories)
    overall_grade = score_to_grade(overall_score)

    report = AnalysisReport(
        target_url=url,
        total_pages=len(pages),
        crawl_duration=crawl_duration,
        categories=categories,
        overall_score=overall_score,
        overall_grade=overall_grade,
        generated_at=now_str(),
        pages=pages,
    )

    # 4. ç”ŸæˆæŠ¥å‘Š
    print()
    generate_report(report, output)
    print(f"\nğŸ‘ ç»¼åˆè¯„åˆ†: {overall_score} ({overall_grade})")
    print(f"ğŸ“Š æµè§ˆå™¨æ‰“å¼€ {output} æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ‘ æ¡ƒæ¡ƒæŠ¤æ³• - ç½‘ç«™åˆ†æçˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ç¤ºä¾‹:
  python main.py https://tenmomo.com
  python main.py https://tenmomo.com --max-pages 100 --delay 0.5 -o report.html
  python main.py https://tenmomo.com --dataforseo-login YOUR_LOGIN --dataforseo-password YOUR_PASSWORD

DataForSEO API:
  æ³¨å†Œ: https://app.dataforseo.com/register
  æ–‡æ¡£: https://docs.dataforseo.com/v3/
  å®šä»·: æŒ‰é‡ä»˜è´¹ï¼Œæ³¨å†Œé€ $1 è¯•ç”¨é¢åº¦ï¼ŒSandbox å…è´¹"""
    )
    parser.add_argument("url", help="ç›®æ ‡ç½‘ç«™ URL")
    parser.add_argument("--max-pages", type=int, default=50, help="æœ€å¤§çˆ¬å–é¡µé¢æ•°ï¼ˆé»˜è®¤ 50ï¼‰")
    parser.add_argument("--delay", type=float, default=1.0, help="è¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰")
    parser.add_argument("-o", "--output", default="", help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ reports/æ—¶é—´æˆ³_tenmomo.htmlï¼‰")
    parser.add_argument("--dataforseo-login", default="",
                        help="DataForSEO API ç™»å½•åï¼ˆæˆ–è®¾ç½® DATAFORSEO_LOGIN ç¯å¢ƒå˜é‡ï¼‰")
    parser.add_argument("--dataforseo-password", default="",
                        help="DataForSEO API å¯†ç ï¼ˆæˆ–è®¾ç½® DATAFORSEO_PASSWORD ç¯å¢ƒå˜é‡ï¼‰")

    args = parser.parse_args()

    if not args.url.startswith(("http://", "https://")):
        args.url = "https://" + args.url

    # ç¯å¢ƒå˜é‡å…œåº•
    dfs_login = args.dataforseo_login or os.environ.get("DATAFORSEO_LOGIN", "")
    dfs_password = args.dataforseo_password or os.environ.get("DATAFORSEO_PASSWORD", "")

    # é»˜è®¤è¾“å‡ºåˆ° reports/ ç›®å½•ï¼Œæ—¶é—´æˆ³å‘½å
    output = args.output
    if not output:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        reports_dir = os.path.join(script_dir, "reports")
        os.makedirs(reports_dir, exist_ok=True)
        domain = urlparse(args.url).netloc.replace(".", "_")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = os.path.join(reports_dir, f"{timestamp}_{domain}.html")

    run_analysis(args.url, args.max_pages, args.delay, output, dfs_login, dfs_password)


if __name__ == "__main__":
    main()
